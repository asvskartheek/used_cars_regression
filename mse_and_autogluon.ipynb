{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import lightgbm as lgb\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "Original = pd.read_csv('data/used_cars.csv')\n",
    "Original[['milage', 'price']] = Original[['milage', 'price']].map(\n",
    "    lambda x: int(''.join(re.findall(r'\\d+', x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                  0\n",
      "brand               0\n",
      "model               0\n",
      "model_year          0\n",
      "milage              0\n",
      "fuel_type        3383\n",
      "engine              0\n",
      "transmission        0\n",
      "ext_col             0\n",
      "int_col             0\n",
      "accident         1632\n",
      "clean_title     14239\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>model_year</th>\n",
       "      <th>milage</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>engine</th>\n",
       "      <th>transmission</th>\n",
       "      <th>ext_col</th>\n",
       "      <th>int_col</th>\n",
       "      <th>accident</th>\n",
       "      <th>clean_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188533</td>\n",
       "      <td>Land</td>\n",
       "      <td>Rover LR2 Base</td>\n",
       "      <td>2015</td>\n",
       "      <td>98000</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>6-Speed A/T</td>\n",
       "      <td>White</td>\n",
       "      <td>Beige</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id brand           model  model_year  milage fuel_type  \\\n",
       "0  188533  Land  Rover LR2 Base        2015   98000  Gasoline   \n",
       "\n",
       "                                         engine transmission ext_col int_col  \\\n",
       "0  240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel  6-Speed A/T   White   Beige   \n",
       "\n",
       "        accident clean_title  \n",
       "0  None reported         Yes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "print(test_df.isna().sum())\n",
    "display(test_df.head(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vehicle_age_features(df):\n",
    "    \"\"\"\n",
    "    Calculate age-related features for vehicles.\n",
    "    \n",
    "    This function performs the following operations:\n",
    "    1. Calculates the age of each vehicle based on the current year (2024) and the model year.\n",
    "    2. Computes the annual milage for each vehicle.\n",
    "    3. Calculates the average milage for each age group.\n",
    "    4. Computes the average annual milage for each age group.\n",
    "    \n",
    "    Features extracted:\n",
    "    - age: The age of the vehicle in years.\n",
    "    - annual_milage: The average number of miles driven per year.\n",
    "    - avg_milage_for_age: The average milage for all vehicles of the same age.\n",
    "    - avg_annual_milage_for_age: The average annual milage for all vehicles of the same age.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): Input dataframe with 'model_year' and 'milage' columns\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with additional age-related features\n",
    "    \"\"\"\n",
    "    current_year = 2024\n",
    "    df['age'] = current_year - df['model_year']\n",
    "    df['annual_milage'] = df['milage'] / df['age']\n",
    "    \n",
    "    df['avg_milage_for_age'] = df.groupby('age')['milage'].transform('mean')\n",
    "    df['avg_annual_milage_for_age'] = df.groupby('age')['annual_milage'].transform('mean')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def identify_luxury_brands(df):\n",
    "    \"\"\"\n",
    "    Identify luxury brands in the dataset and create a binary indicator.\n",
    "    \n",
    "    This function performs the following operations:\n",
    "    1. Defines a set of luxury car brands.\n",
    "    2. Creates a binary indicator column 'is_luxury' based on whether the brand is in the luxury set.\n",
    "    \n",
    "    Features extracted:\n",
    "    - is_luxury: Binary indicator (0 or 1) representing whether the brand is considered luxury.\n",
    "    \n",
    "    Modifications:\n",
    "    - Adds a new column 'is_luxury' to the dataframe.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): Input dataframe with 'brand' column\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with additional luxury brand indicator\n",
    "    \"\"\"\n",
    "    luxury_brands = {\n",
    "        'Mercedes-Benz', 'BMW', 'Audi', 'Porsche', 'Land Rover', \n",
    "        'Lexus', 'Jaguar', 'Bentley', 'Maserati', 'Lamborghini', \n",
    "        'Rolls-Royce', 'Ferrari', 'McLaren', 'Aston Martin', 'Maybach'\n",
    "    }\n",
    "    df['is_luxury'] = df['brand'].isin(luxury_brands).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def enrich_dataset(df):\n",
    "    \"\"\"\n",
    "    Enrich the dataset with all additional features.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Enriched dataframe with all additional features\n",
    "    \"\"\"\n",
    "    df = calculate_vehicle_age_features(df)\n",
    "    df = identify_luxury_brands(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Enrich both training and testing datasets\n",
    "train_enriched = enrich_dataset(train)\n",
    "test_enriched = enrich_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_categorical_features(dataframe, threshold=100):\n",
    "    \"\"\"\n",
    "    Preprocess categorical features in the dataframe.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    1. Replaces low-frequency categories with 'noise' for specified columns.\n",
    "    2. Fills missing values with 'missing' for all categorical columns.\n",
    "    3. Converts categorical columns to 'category' dtype.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): Input dataframe to be processed.\n",
    "    threshold (int): Minimum frequency for a category to be kept. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Processed dataframe with updated categorical features.\n",
    "    \"\"\"\n",
    "    \n",
    "    categorical_columns = [\n",
    "        'brand', 'model', 'fuel_type', 'engine', 'transmission',\n",
    "        'ext_col', 'int_col', 'accident', 'clean_title'\n",
    "    ]\n",
    "    columns_to_reduce = ['model', 'engine', 'transmission', 'ext_col', 'int_col']\n",
    "    \n",
    "    for column in columns_to_reduce:\n",
    "        mask = dataframe[column].value_counts(dropna=False)[dataframe[column]].values < threshold\n",
    "        dataframe.loc[mask, column] = \"noise\"\n",
    "        \n",
    "    for column in categorical_columns:\n",
    "        dataframe[column] = dataframe[column].fillna('missing')\n",
    "        dataframe[column] = dataframe[column].astype('category')\n",
    "        \n",
    "    return dataframe\n",
    "\n",
    "train_processed = preprocess_categorical_features(train_enriched)\n",
    "test_processed = preprocess_categorical_features(test_enriched)\n",
    "\n",
    "X_features = train_processed.drop('price', axis=1)\n",
    "y_target = train_processed['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of MAE and the difference between MAE and MSE. Thanks to @Backpacker for this idea.\n",
    "OG comment: the best way to improve predictions is to handle outliers in the training data without removing them. one of participant did this well by using both MSE (robust to outliers) and MAE (less robust) to train two different models then used the difference between the models as a new feature to detect outliers. [Link](https://www.kaggle.com/competitions/playground-series-s4e9/discussion/536456)\n",
    "\n",
    "# Innovative Outlier Handling: Combining MSE and MAE\n",
    "\n",
    "1. MSE vs MAE:\n",
    "    - MSE is more sensitive to outliers because it squares the errors, amplifying large deviations.\n",
    "    - MAE is less sensitive to outliers as it uses absolute values, treating all deviations linearly.\n",
    "\n",
    "2. Training two models:\n",
    "    - One model is trained using MSE as the loss function.\n",
    "    - Another model is trained using MAE as the loss function.\n",
    "\n",
    "3. Detecting outliers:\n",
    "    - The difference between the predictions of these two models can indicate potential outliers.\n",
    "    - For normal data points, both models should predict similar values.\n",
    "    - For outliers, the MSE model's prediction might deviate more from the MAE model's prediction.\n",
    "\n",
    "4. Creating a new feature:\n",
    "    - The difference between the MSE and MAE model predictions becomes a new feature.\n",
    "    - This feature essentially quantifies the \"outlierness\" of each data point.\n",
    "\n",
    "5. Handling outliers without removal:\n",
    "    - Instead of removing outliers, which could lose valuable information, this approach allows the final model to learn from the \"outlierness\" feature.\n",
    "    - The model can then adjust its predictions based on how likely a data point is to be an outlier.\n",
    "\n",
    "This method is particularly clever because it:\n",
    "    - Preserves all data points, including potential outliers.\n",
    "    - Provides a data-driven way to identify and handle outliers.\n",
    "    - Adds valuable information to the model without making strong assumptions about what constitutes an outlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['brand', 'model', 'fuel_type', 'engine', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
      "Training fold 1/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1957\n",
      "[LightGBM] [Info] Number of data points in the train set: 150826, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 30825.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttrain's l1: 16527\tvalid's l1: 16659.9\n",
      "Early stopping, best iteration is:\n",
      "[320]\ttrain's l1: 16512.2\tvalid's l1: 16659.4\n",
      "LGBM Fold RMSE: 63150.238787390896\n",
      "Training fold 2/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 150826, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 30825.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttrain's l1: 16435.4\tvalid's l1: 17032.9\n",
      "Early stopping, best iteration is:\n",
      "[312]\ttrain's l1: 16421.7\tvalid's l1: 17032\n",
      "LGBM Fold RMSE: 73999.04347168155\n",
      "Training fold 3/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 150826, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 30798.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttrain's l1: 16258.6\tvalid's l1: 17780.9\n",
      "[600]\ttrain's l1: 16009.7\tvalid's l1: 17777.8\n",
      "Early stopping, best iteration is:\n",
      "[500]\ttrain's l1: 16085.1\tvalid's l1: 17775.1\n",
      "LGBM Fold RMSE: 82703.62210648022\n",
      "Training fold 4/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1957\n",
      "[LightGBM] [Info] Number of data points in the train set: 150827, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 30900.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttrain's l1: 16313.2\tvalid's l1: 17537.3\n",
      "Early stopping, best iteration is:\n",
      "[147]\ttrain's l1: 16493.8\tvalid's l1: 17533.6\n",
      "LGBM Fold RMSE: 76471.65516507316\n",
      "Training fold 5/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1957\n",
      "[LightGBM] [Info] Number of data points in the train set: 150827, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 30750.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttrain's l1: 16455.3\tvalid's l1: 16994.4\n",
      "Early stopping, best iteration is:\n",
      "[363]\ttrain's l1: 16392.5\tvalid's l1: 16991.3\n",
      "LGBM Fold RMSE: 70994.12484482271\n",
      "Mean RMSE: 73463.7368750897\n",
      "Categorical columns: ['brand', 'model', 'fuel_type', 'engine', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
      "Training fold 1/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2212\n",
      "[LightGBM] [Info] Number of data points in the train set: 150826, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 44031.757197\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttrain's l2: 5.22718e+09\tvalid's l2: 3.92932e+09\n",
      "LGBM Fold RMSE: 62684.285708337906\n",
      "Training fold 2/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2213\n",
      "[LightGBM] [Info] Number of data points in the train set: 150826, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 43925.459689\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttrain's l2: 4.84333e+09\tvalid's l2: 5.38576e+09\n",
      "LGBM Fold RMSE: 73387.74596833072\n",
      "Training fold 3/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2213\n",
      "[LightGBM] [Info] Number of data points in the train set: 150826, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 43728.734588\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttrain's l2: 4.49064e+09\tvalid's l2: 6.71971e+09\n",
      "LGBM Fold RMSE: 81973.8543850994\n",
      "Training fold 4/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2212\n",
      "[LightGBM] [Info] Number of data points in the train set: 150827, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 43823.090242\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttrain's l2: 4.67619e+09\tvalid's l2: 5.72751e+09\n",
      "LGBM Fold RMSE: 75680.31351564355\n",
      "Training fold 5/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2212\n",
      "[LightGBM] [Info] Number of data points in the train set: 150827, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 43881.039515\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttrain's l2: 4.78014e+09\tvalid's l2: 4.93449e+09\n",
      "LGBM Fold RMSE: 70245.94908328413\n",
      "Mean RMSE: 72794.42973213914\n",
      "       id brand                 model  model_year  milage fuel_type  \\\n",
      "0  188533  Land                 noise        2015   98000  Gasoline   \n",
      "1  188534  Land     Rover Defender SE        2020    9142    Hybrid   \n",
      "2  188535  Ford    Expedition Limited        2022   28121  Gasoline   \n",
      "3  188536  Audi                 noise        2016   61258  Gasoline   \n",
      "4  188537  Audi  A6 2.0T Premium Plus        2018   59000  Gasoline   \n",
      "\n",
      "                                              engine        transmission  \\\n",
      "0       240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel         6-Speed A/T   \n",
      "1  395.0HP 3.0L Straight 6 Cylinder Engine Gasoli...         8-Speed A/T   \n",
      "2                    3.5L V6 24V PDI DOHC Twin Turbo  10-Speed Automatic   \n",
      "3                                     2.0 Liter TFSI           Automatic   \n",
      "4       252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel                 A/T   \n",
      "\n",
      "  ext_col int_col       accident clean_title  age  annual_milage  \\\n",
      "0   White   Beige  None reported         Yes    9   10888.888889   \n",
      "1  Silver   Black  None reported         Yes    4    2285.500000   \n",
      "2   White   Ebony  None reported     missing    2   14060.500000   \n",
      "3   noise   Black  None reported     missing    8    7657.250000   \n",
      "4    Gray   Black  None reported         Yes    6    9833.333333   \n",
      "\n",
      "   avg_milage_for_age  avg_annual_milage_for_age  is_luxury      MAE_pred  \\\n",
      "0        81078.503981                9008.722665          0  16561.711113   \n",
      "1        34258.886442                8564.721611          0  58399.276141   \n",
      "2        17877.043403                8938.521702          0  49207.584176   \n",
      "3        75999.679762                9499.959970          1  24914.177773   \n",
      "4        52105.532436                8684.255406          1  27808.658571   \n",
      "\n",
      "    outlierness  \n",
      "0   3641.585516  \n",
      "1  16022.838626  \n",
      "2   9557.524434  \n",
      "3   4359.004652  \n",
      "4   4100.891136  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, model_type='LGBM', objective='MAE', cat_cols=None):\n",
    "    \"\"\"\n",
    "    Train a model (LightGBM or CatBoost) and return predictions for validation data.\n",
    "    \n",
    "    This function trains either a LightGBM or CatBoost model based on the specified parameters\n",
    "    and returns the trained model along with predictions for the validation data.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target values.\n",
    "        X_val (pd.DataFrame): Validation features.\n",
    "        y_val (pd.Series): Validation target values.\n",
    "        model_type (str, optional): Type of model to use ('LGBM' for LightGBM or 'CAT' for CatBoost). Defaults to 'LGBM'.\n",
    "        objective (str, optional): Objective function to use ('MAE' or 'MSE'). Defaults to 'MAE'.\n",
    "        cat_cols (list, optional): List of categorical column names. Required for CatBoost. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - trained model: The trained LightGBM or CatBoost model.\n",
    "            - np.array: Predictions for validation data.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If an invalid model_type is specified.\n",
    "    \n",
    "    Example:\n",
    "        >>> X_train = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': ['A', 'B', 'C']})\n",
    "        >>> y_train = pd.Series([10, 20, 30])\n",
    "        >>> X_val = pd.DataFrame({'feature1': [4, 5], 'feature2': ['B', 'C']})\n",
    "        >>> y_val = pd.Series([40, 50])\n",
    "        >>> model, val_pred = train_model(X_train, y_train, X_val, y_val, model_type='LGBM', objective='MAE')\n",
    "    \"\"\"\n",
    "    if model_type == 'LGBM':\n",
    "        params = {\n",
    "            'objective': objective,\n",
    "            'n_estimators': 1000,\n",
    "            'random_state': 1,\n",
    "        }\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "        callbacks = [lgb.log_evaluation(period=300), lgb.early_stopping(stopping_rounds=200)]\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[train_data, val_data],\n",
    "            valid_names=['train', 'valid'],\n",
    "            callbacks=callbacks    \n",
    "        )\n",
    "        val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    elif model_type == 'CAT':\n",
    "        params = {\n",
    "            'loss_function': objective,\n",
    "            'iterations': 1000,\n",
    "            'random_seed': 1,\n",
    "            'early_stopping_rounds': 200\n",
    "        }\n",
    "        train_data = Pool(data=X_train, label=y_train, cat_features=cat_cols)\n",
    "        val_data = Pool(data=X_val, label=y_val, cat_features=cat_cols)\n",
    "        \n",
    "        model = CatBoostRegressor(**params)\n",
    "        model.fit(train_data, eval_set=val_data, verbose=150)\n",
    "        \n",
    "        val_pred = model.predict(X_val)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose either 'LGBM' or 'CAT'.\")\n",
    "    \n",
    "    return model, val_pred\n",
    "\n",
    "def cross_validate_and_predict(X_features, y_target, test_data, model_type='LGBM', objective='MAE'):\n",
    "    \"\"\"\n",
    "    Perform cross-validation, train models, and make predictions on test data.\n",
    "    \n",
    "    This function implements a 5-fold cross-validation strategy, trains models on each fold,\n",
    "    and generates out-of-fold predictions for the training data as well as predictions for the test data.\n",
    "    \n",
    "    Args:\n",
    "        X_features (pd.DataFrame): Feature matrix for training data.\n",
    "        y_target (pd.Series): Target variable for training data.\n",
    "        test_data (pd.DataFrame): Processed test data.\n",
    "        model_type (str, optional): Type of model to use ('LGBM' or 'CAT'). Defaults to 'LGBM'.\n",
    "        objective (str, optional): Objective function to use ('MAE' or 'MSE'). Defaults to 'MAE'.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - np.array: Out-of-fold predictions for training data.\n",
    "            - np.array: Predictions for test data.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If an invalid model_type is specified.\n",
    "    \n",
    "    Example:\n",
    "        >>> X_features = pd.DataFrame({'feature1': range(100), 'feature2': ['A']*50 + ['B']*50})\n",
    "        >>> y_target = pd.Series(range(100))\n",
    "        >>> test_data = pd.DataFrame({'feature1': range(20), 'feature2': ['A']*10 + ['B']*10})\n",
    "        >>> oof_pred, test_pred = cross_validate_and_predict(X_features, y_target, test_data, model_type='LGBM', objective='MAE')\n",
    "    \"\"\"\n",
    "    cat_cols = X_features.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    print(f\"Categorical columns: {cat_cols}\")\n",
    "    \n",
    "    oof_predictions = np.zeros(len(X_features))\n",
    "    test_predictions = np.zeros(len(test_data))\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_features)):\n",
    "        print(f\"Training fold {fold + 1}/5 with {model_type}\")\n",
    "\n",
    "        X_train, X_val = X_features.iloc[train_idx], X_features.iloc[val_idx]\n",
    "        y_train, y_val = y_target.iloc[train_idx], y_target.iloc[val_idx]\n",
    "\n",
    "        model, val_pred = train_model(X_train, y_train, X_val, y_val, model_type, objective, cat_cols)\n",
    "        \n",
    "        if model_type == 'LGBM':\n",
    "            test_pred = model.predict(test_data, num_iteration=model.best_iteration)\n",
    "        elif model_type == 'CAT':\n",
    "            test_pred = model.predict(test_data)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose either 'LGBM' or 'CAT'.\")\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "\n",
    "        print(f'{model_type} Fold RMSE: {rmse}')\n",
    "        \n",
    "        oof_predictions[val_idx] = val_pred\n",
    "        test_predictions += test_pred / 5\n",
    "    \n",
    "    print(f'Mean RMSE: {np.mean(rmse_scores)}')\n",
    "    return oof_predictions, test_predictions\n",
    "\n",
    "def get_outlierness(X_features, y_target, test_data, model_type='LGBM'):\n",
    "    \"\"\"\n",
    "    Calculate the 'outlierness' of data points by comparing MSE and MAE predictions.\n",
    "    \n",
    "    This function trains models using both MAE and MSE objectives, then calculates\n",
    "    the difference between their predictions to create an 'outlierness' feature.\n",
    "    This feature can help identify potential outliers in the data.\n",
    "    \n",
    "    Args:\n",
    "        X_features (pd.DataFrame): Feature matrix for training data.\n",
    "        y_target (pd.Series): Target variable for training data.\n",
    "        test_data (pd.DataFrame): Processed test data.\n",
    "        model_type (str, optional): Type of model to use ('LGBM' or 'CAT'). Defaults to 'LGBM'.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pd.DataFrame: Training data with added 'MAE_pred' and 'outlierness' features.\n",
    "            - pd.DataFrame: Test data with added 'MAE_pred' and 'outlierness' features.\n",
    "    \n",
    "    Note:\n",
    "        The 'outlierness' feature is calculated as the difference between\n",
    "        MSE-based predictions and MAE-based predictions. This calculation is done\n",
    "        for both training and test data. For training data, we use out-of-fold\n",
    "        predictions, while for test data, we use the average predictions from\n",
    "        all folds. This means the 'outlierness' values may have slightly different\n",
    "        distributions between train and test sets due to the different prediction methods.\n",
    "    \n",
    "    Example:\n",
    "        >>> X_features = pd.DataFrame({'feature1': range(100), 'feature2': ['A']*50 + ['B']*50})\n",
    "        >>> y_target = pd.Series(range(100))\n",
    "        >>> test_data = pd.DataFrame({'feature1': range(20), 'feature2': ['A']*10 + ['B']*10})\n",
    "        >>> X_features_with_outlierness, test_data_with_outlierness = get_outlierness(X_features, y_target, test_data, 'LGBM')\n",
    "    \"\"\"\n",
    "    # Train and predict using MAE objective\n",
    "    oof_mae, test_mae = cross_validate_and_predict(X_features, y_target, test_data, model_type, 'MAE')\n",
    "    X_features['MAE_pred'] = oof_mae\n",
    "    test_data['MAE_pred'] = test_mae\n",
    "\n",
    "    # Train and predict using MSE objective\n",
    "    oof_mse, test_mse = cross_validate_and_predict(X_features, y_target, test_data, model_type, 'MSE')\n",
    "\n",
    "    # Calculate the difference between MSE and MAE predictions (outlierness)\n",
    "    X_features['outlierness'] = oof_mse - X_features['MAE_pred']\n",
    "    test_data['outlierness'] = test_mse - test_data['MAE_pred']\n",
    "\n",
    "    return X_features, test_data\n",
    "\n",
    "# Example usage:\n",
    "X_features_with_outlierness, test_processed_with_outlierness = get_outlierness(X_features, y_target, test_processed, 'LGBM')\n",
    "print(test_processed_with_outlierness.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240927_184022\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.2.0: Wed Nov 15 21:59:33 PST 2023; root:xnu-10002.61.3~2/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       1.54 GB / 8.00 GB (19.3%)\n",
      "Disk Space Avail:   99.32 GB / 460.43 GB (21.6%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "2024-09-28 00:10:22,931\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-09-28 00:10:25,006\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"AutogluonModels/ag-20240927_184022/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Beginning AutoGluon training ... Time limit = 147s\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m AutoGluon will save models to \"AutogluonModels/ag-20240927_184022/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Train Data Rows:    167584\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Train Data Columns: 19\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Label Column:       price\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tAvailable Memory:                    1416.41 MB\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tTrain Data (Original)  Memory Usage: 14.63 MB (1.0% of available memory)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\t('category', []) : 9 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\t('float', [])    : 5 | ['annual_milage', 'avg_milage_for_age', 'avg_annual_milage_for_age', 'MAE_pred', 'outlierness']\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\t('int', [])      : 5 | ['id', 'model_year', 'milage', 'age', 'is_luxury']\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\t('category', [])  : 8 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\t('float', [])     : 5 | ['annual_milage', 'avg_milage_for_age', 'avg_annual_milage_for_age', 'MAE_pred', 'outlierness']\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\t('int', [])       : 4 | ['id', 'model_year', 'milage', 'age']\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t\t('int', ['bool']) : 2 | ['clean_title', 'is_luxury']\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.2s = Fit runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t19 features in original data used to generate 19 features in processed data.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tTrain Data (Processed) Memory Usage: 13.43 MB (1.0% of available memory)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Included models: ['GBM', 'CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting 36 L1 models ...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 97.88s of the 146.85s of remaining time.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.42%)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t-73172.7007\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t2.11s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.54s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 93.19s of the 142.17s of remaining time.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.54%)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t-73425.1024\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t2.33s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.29s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 89.17s of the 138.14s of remaining time.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.57%)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t-73076.7193\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t71.41s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.36s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 16.35s of the 65.32s of remaining time.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.38%)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t-73938.3437\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t2.46s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.71s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 12.28s of the 61.25s of remaining time.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.79%)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t-73134.6518\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t9.81s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.21s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 0.97s of the 49.94s of remaining time.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.92%)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t-76733.1481\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t1.14s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.22s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 146.85s of the 47.03s of remaining time.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L1': 0.667, 'LightGBMXT_BAG_L1': 0.292, 'LightGBM_BAG_L1': 0.042}\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t-73046.0771\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.08s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Included models: ['GBM', 'CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting 36 L2 models ...\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 46.94s of the 46.91s of remaining time.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.10%)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t-73109.3329\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t4.6s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.8s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 40.4s of the 40.37s of remaining time.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.53%)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t-73356.1711\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t2.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.3s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 36.91s of the 36.89s of remaining time.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.24%)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t-73067.1494\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t29.67s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.44s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 5.64s of the 5.61s of remaining time.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.59%)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t-73744.3802\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t4.67s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.95s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 146.85s of the -1.4s of remaining time.\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L1': 0.467, 'LightGBMXT_BAG_L2': 0.4, 'CatBoost_BAG_L2': 0.067, 'LightGBMLarge_BAG_L2': 0.067}\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t-72995.2445\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.09s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m AutoGluon training complete, total runtime = 148.51s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 4633.0 rows/s (20948 batch size)\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240927_184022/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=25307)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                   model  score_holdout     score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    WeightedEnsemble_L3  -69765.631764 -72995.244492  root_mean_squared_error        1.466188       4.523349  128.294574                 0.001202                0.002138           0.085362            3       True         12\n",
      "1    WeightedEnsemble_L2  -69772.429259 -73046.077089  root_mean_squared_error        0.637413       1.190545   75.945810                 0.001286                0.001821           0.084048            2       True          7\n",
      "2        CatBoost_BAG_L2  -69790.941472 -73067.149394  root_mean_squared_error        1.085747       2.774000  118.941305                 0.122370                0.440864          29.667975            2       True         10\n",
      "3      LightGBMXT_BAG_L2  -69796.360912 -73109.332890  root_mean_squared_error        1.158873       3.129221   93.870472                 0.195496                0.796084           4.597143            2       True          8\n",
      "4        CatBoost_BAG_L1  -69818.721277 -73076.719268  root_mean_squared_error        0.313176       0.355953   71.414767                 0.313176                0.355953          71.414767            1       True          3\n",
      "5      LightGBMXT_BAG_L1  -69828.764798 -73172.700733  root_mean_squared_error        0.231156       0.542022    2.114275                 0.231156                0.542022           2.114275            1       True          1\n",
      "6   CatBoost_r177_BAG_L1  -69873.346087 -73134.651757  root_mean_squared_error        0.104076       0.209906    9.814528                 0.104076                0.209906           9.814528            1       True          5\n",
      "7        LightGBM_BAG_L1  -70030.276529 -73425.102357  root_mean_squared_error        0.091795       0.290748    2.332720                 0.091795                0.290748           2.332720            1       True          2\n",
      "8        LightGBM_BAG_L2  -70042.175321 -73356.171147  root_mean_squared_error        1.127719       2.637827   91.286088                 0.164342                0.304690           2.012758            2       True          9\n",
      "9   LightGBMLarge_BAG_L2  -70283.177625 -73744.380158  root_mean_squared_error        1.147120       3.284262   93.944093                 0.183743                0.951126           4.670764            2       True         11\n",
      "10  LightGBMLarge_BAG_L1  -70371.240981 -73938.343666  root_mean_squared_error        0.158810       0.714939    2.458494                 0.158810                0.714939           2.458494            1       True          4\n",
      "11  LightGBM_r131_BAG_L1  -73614.026800 -76733.148126  root_mean_squared_error        0.064364       0.219568    1.138546                 0.064364                0.219568           1.138546            1       True          6\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t155s\t = DyStack   runtime |\t445s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 445s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240927_184022\"\n",
      "Train Data Rows:    188533\n",
      "Train Data Columns: 19\n",
      "Label Column:       price\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3267.10 MB\n",
      "\tTrain Data (Original)  Memory Usage: 16.45 MB (0.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 9 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\t\t('float', [])    : 5 | ['annual_milage', 'avg_milage_for_age', 'avg_annual_milage_for_age', 'MAE_pred', 'outlierness']\n",
      "\t\t('int', [])      : 5 | ['id', 'model_year', 'milage', 'age', 'is_luxury']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 8 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\t\t('float', [])     : 5 | ['annual_milage', 'avg_milage_for_age', 'avg_annual_milage_for_age', 'MAE_pred', 'outlierness']\n",
      "\t\t('int', [])       : 4 | ['id', 'model_year', 'milage', 'age']\n",
      "\t\t('int', ['bool']) : 2 | ['clean_title', 'is_luxury']\n",
      "\t0.2s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 15.11 MB (0.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Included models: ['GBM', 'CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 36 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 296.53s of the 444.9s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.17%)\n",
      "\t-72759.2948\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.27s\t = Training   runtime\n",
      "\t0.82s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 291.87s of the 440.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.03%)\n",
      "\t-72943.4569\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 288.21s of the 436.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.33%)\n",
      "\t-72701.451\t = Validation score   (-root_mean_squared_error)\n",
      "\t103.38s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 183.27s of the 331.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.00%)\n",
      "\t-73293.5947\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.53s\t = Training   runtime\n",
      "\t1.03s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 177.65s of the 326.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.84%)\n",
      "\t-72712.3457\t = Validation score   (-root_mean_squared_error)\n",
      "\t51.17s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 124.69s of the 273.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.75%)\n",
      "\t-72826.9428\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.34s\t = Training   runtime\n",
      "\t3.73s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 113.81s of the 262.17s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.79%)\n",
      "\t-72727.3405\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.18s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 76.18s of the 224.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.37%)\n",
      "\t-72701.624\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.59s\t = Training   runtime\n",
      "\t4.66s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 61.99s of the 210.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.74%)\n",
      "\t-72707.3508\t = Validation score   (-root_mean_squared_error)\n",
      "\t49.67s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 10.88s of the 159.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.55%)\n",
      "\t-75796.1597\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.67s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 0.02s of the 148.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=8.36%)\n",
      "\tTime limit exceeded... Skipping LightGBM_r188_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 147.52s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_r96_BAG_L1': 0.35, 'CatBoost_r9_BAG_L1': 0.25, 'LightGBM_r131_BAG_L1': 0.15, 'CatBoost_BAG_L1': 0.1, 'CatBoost_r137_BAG_L1': 0.1, 'CatBoost_r177_BAG_L1': 0.05}\n",
      "\t-72613.4714\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Included models: ['GBM', 'CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 36 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 147.39s of the 147.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.26%)\n",
      "\t-72707.6301\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.51s\t = Training   runtime\n",
      "\t1.16s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 137.67s of the 137.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.71%)\n",
      "\t-73002.3066\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.02s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 133.12s of the 133.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.12%)\n",
      "\t-72633.3265\t = Validation score   (-root_mean_squared_error)\n",
      "\t83.09s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 48.52s of the 48.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.43%)\n",
      "\t-73505.8336\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.96s\t = Training   runtime\n",
      "\t0.92s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 41.33s of the 41.3s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.34%)\n",
      "\t-72637.8714\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.29s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 6.52s of the 6.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.15%)\n",
      "\t-73001.3693\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.61s\t = Training   runtime\n",
      "\t2.88s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -1.94s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.32, 'CatBoost_BAG_L2': 0.2, 'CatBoost_r177_BAG_L2': 0.2, 'LightGBM_r96_BAG_L1': 0.12, 'CatBoost_BAG_L1': 0.08, 'CatBoost_r177_BAG_L1': 0.04, 'LightGBM_r131_BAG_L1': 0.04}\n",
      "\t-72580.2431\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 447.35s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1635.9 rows/s (23567 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240927_184022\")\n"
     ]
    }
   ],
   "source": [
    "X_features_with_outlierness['price'] = y_target\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label='price',\n",
    "    eval_metric='rmse',\n",
    "    problem_type='regression'\n",
    ").fit(\n",
    "    X_features_with_outlierness,\n",
    "    presets='best_quality',\n",
    "    time_limit=600, # in seconds\n",
    "    verbosity=2,\n",
    "    num_gpus=0,\n",
    "    included_model_types=['GBM', 'CAT']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>model_year</th>\n",
       "      <th>milage</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>engine</th>\n",
       "      <th>transmission</th>\n",
       "      <th>ext_col</th>\n",
       "      <th>int_col</th>\n",
       "      <th>accident</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>age</th>\n",
       "      <th>annual_milage</th>\n",
       "      <th>avg_milage_for_age</th>\n",
       "      <th>avg_annual_milage_for_age</th>\n",
       "      <th>is_luxury</th>\n",
       "      <th>MAE_pred</th>\n",
       "      <th>outlierness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188533</td>\n",
       "      <td>Land</td>\n",
       "      <td>noise</td>\n",
       "      <td>2015</td>\n",
       "      <td>98000</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>6-Speed A/T</td>\n",
       "      <td>White</td>\n",
       "      <td>Beige</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>9</td>\n",
       "      <td>10888.888889</td>\n",
       "      <td>81078.503981</td>\n",
       "      <td>9008.722665</td>\n",
       "      <td>0</td>\n",
       "      <td>16561.711113</td>\n",
       "      <td>3641.585516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id brand  model  model_year  milage fuel_type  \\\n",
       "0  188533  Land  noise        2015   98000  Gasoline   \n",
       "\n",
       "                                         engine transmission ext_col int_col  \\\n",
       "0  240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel  6-Speed A/T   White   Beige   \n",
       "\n",
       "        accident clean_title  age  annual_milage  avg_milage_for_age  \\\n",
       "0  None reported         Yes    9   10888.888889        81078.503981   \n",
       "\n",
       "   avg_annual_milage_for_age  is_luxury      MAE_pred  outlierness  \n",
       "0                9008.722665          0  16561.711113  3641.585516  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_processed_with_outlierness.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'brand', 'model', 'model_year', 'milage', 'fuel_type', 'engine',\n",
       "       'transmission', 'ext_col', 'int_col', 'accident', 'clean_title', 'age',\n",
       "       'annual_milage', 'avg_milage_for_age', 'avg_annual_milage_for_age',\n",
       "       'is_luxury', 'MAE_pred', 'outlierness'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_processed_with_outlierness.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions...\n",
      "Out-of-fold predictions saved to predictions/oof_autogluon_20240928_002127.csv\n",
      "Test predictions saved to predictions/submission_autogluon_20240928_002127.csv\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor.load(\"AutogluonModels/ag-20240927_184022\")\n",
    "test_predictions = predictor.predict(test_processed_with_outlierness)\n",
    "\n",
    "# Save predictions\n",
    "print(\"Saving predictions...\")\n",
    "\n",
    "# Save out-of-fold predictions\n",
    "oof_predictions = predictor.predict(X_features_with_outlierness)\n",
    "oof_df = X_features_with_outlierness[['id']].copy()\n",
    "oof_df['pred'] = oof_predictions\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "oof_path = f\"predictions/oof_autogluon_{timestamp}.csv\"\n",
    "oof_df.to_csv(oof_path, index=False)\n",
    "print(f\"Out-of-fold predictions saved to {oof_path}\")\n",
    "\n",
    "# Save test predictions\n",
    "sub = pd.read_csv(\"data/sample_submission.csv\")\n",
    "sub['price'] = test_predictions\n",
    "sub_path = f\"predictions/submission_autogluon_{timestamp}.csv\"\n",
    "sub.to_csv(sub_path, index=False)\n",
    "print(f\"Test predictions saved to {sub_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
